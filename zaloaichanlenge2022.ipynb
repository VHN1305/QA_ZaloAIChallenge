{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6049929,"sourceType":"datasetVersion","datasetId":3412452}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:21.578698Z","iopub.execute_input":"2024-01-27T06:38:21.579094Z","iopub.status.idle":"2024-01-27T06:38:37.069011Z","shell.execute_reply.started":"2024-01-27T06:38:21.579060Z","shell.execute_reply":"2024-01-27T06:38:37.068043Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.35.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.24.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.17.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (10.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=55949b9db389cf175f573c4a91d09c05fb6c0def9dcbf5d1cf21f10efabbee31\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# ! pip install py-vncorenlp","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:37.071382Z","iopub.execute_input":"2024-01-27T06:38:37.072041Z","iopub.status.idle":"2024-01-27T06:38:37.076437Z","shell.execute_reply.started":"2024-01-27T06:38:37.071977Z","shell.execute_reply":"2024-01-27T06:38:37.075554Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Retrieval","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nfrom sentence_transformers import SentenceTransformer, util, CrossEncoder\nfrom sentence_transformers import InputExample, SentenceTransformer, losses\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\nfrom tqdm import tqdm\nfrom transformers import DistilBertForQuestionAnswering\nfrom transformers import DistilBertTokenizerFast","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-27T06:38:37.077716Z","iopub.execute_input":"2024-01-27T06:38:37.077982Z","iopub.status.idle":"2024-01-27T06:38:40.833470Z","shell.execute_reply.started":"2024-01-27T06:38:37.077958Z","shell.execute_reply":"2024-01-27T06:38:40.832552Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = pd.read_json(\"/kaggle/input/e2eqa-wiki-zalo-ai/e2eqa-trainpublic_test-v1/e2eqa-train+public_test-v1/zac2022_train_merged_final.json\")['data']\ntest_data = pd.read_json(\"/kaggle/input/e2eqa-wiki-zalo-ai/e2eqa-trainpublic_test-v1/e2eqa-train+public_test-v1/zac2022_testa_sample_submission.json\")['data']","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:40.834780Z","iopub.execute_input":"2024-01-27T06:38:40.835389Z","iopub.status.idle":"2024-01-27T06:38:41.355104Z","shell.execute_reply.started":"2024-01-27T06:38:40.835352Z","shell.execute_reply":"2024-01-27T06:38:41.354300Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data[0]","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:41.358186Z","iopub.execute_input":"2024-01-27T06:38:41.358512Z","iopub.status.idle":"2024-01-27T06:38:41.365854Z","shell.execute_reply.started":"2024-01-27T06:38:41.358482Z","shell.execute_reply":"2024-01-27T06:38:41.364920Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'id': '718d41cd997b2b44b0685ac54aa55bd8',\n 'question': 'Thủ tướng Trung Quốc là gì',\n 'title': 'Trung Quốc',\n 'text': 'Thủ tướng Trung Quốc là nhân vật lãnh đạo chính phủ , chủ trì Quốc vụ viện gồm bốn phó thủ tướng cùng người đứng đầu các bộ và uỷ ban cấp bộ . Chủ tịch nước đương nhiệm là Tập Cận Bình , ông cũng là Tổng Bí thư của Đảng Cộng sản Trung Quốc và Chủ tịch Quân uỷ Trung Quốc , do vậy ông là lãnh đạo tối cao của Trung Quốc .',\n 'category': 'PARTIAL_ANNOTATION',\n 'is_long_answer': True}"},"metadata":{}}]},{"cell_type":"code","source":"train_data[1]","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:41.367032Z","iopub.execute_input":"2024-01-27T06:38:41.367322Z","iopub.status.idle":"2024-01-27T06:38:41.379714Z","shell.execute_reply.started":"2024-01-27T06:38:41.367298Z","shell.execute_reply":"2024-01-27T06:38:41.378725Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'id': 'c926e7b0717202618a10dd907d4b4c39',\n 'question': 'Đất nước nào không có quân đội',\n 'title': '',\n 'text': 'có 23 quốc gia không có lực lượng quân đội, bao gồm: Costa Rica, Iceland, Panama, Micronesia, Quần đảo Marshall, và Vatican...',\n 'short_candidate_start': 53,\n 'short_candidate': 'Costa Rica, Iceland, Panama, Micronesia, Quần đảo Marshall, và Vatican...',\n 'answer': 'wiki/Danh_sách_quốc_gia_không_có_lực_lượng_vũ_trang',\n 'category': 'FULL_ANNOTATION',\n 'is_long_answer': True}"},"metadata":{}}]},{"cell_type":"code","source":"all_id = []\ntrain_questions = []\ntitle = []\ntrain_contexts = []\ncategory = []\nlong_answer = []\nshort_candidate_start = []\nshort_candidate = []\n\nfor item in train_data:\n    all_id.append(item['id'])\n    train_questions.append(item['question'])\n    title.append(item['title'])\n    train_contexts.append(item['text'])\n    category.append(item['category'])\n    long_answer.append(1 if item['is_long_answer'] else 0)\n    short_candidate_start.append(item['short_candidate_start'] if 'short_candidate_start' in item else None)\n    short_candidate.append(item['short_candidate'] if 'short_candidate' in item else None)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:41.381005Z","iopub.execute_input":"2024-01-27T06:38:41.381580Z","iopub.status.idle":"2024-01-27T06:38:41.422669Z","shell.execute_reply.started":"2024-01-27T06:38:41.381546Z","shell.execute_reply":"2024-01-27T06:38:41.422022Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['id'] = all_id\ndf['context'] = train_contexts\ndf['title'] = title\ndf['question'] = train_questions\ndf['category'] = category\ndf['answer'] = long_answer\ndf['short_candidate_start'] = short_candidate_start\ndf['short_candidate'] = short_candidate","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:41.423770Z","iopub.execute_input":"2024-01-27T06:38:41.424142Z","iopub.status.idle":"2024-01-27T06:38:41.464269Z","shell.execute_reply.started":"2024-01-27T06:38:41.424109Z","shell.execute_reply":"2024-01-27T06:38:41.463307Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:41.465396Z","iopub.execute_input":"2024-01-27T06:38:41.465649Z","iopub.status.idle":"2024-01-27T06:38:41.484304Z","shell.execute_reply.started":"2024-01-27T06:38:41.465627Z","shell.execute_reply":"2024-01-27T06:38:41.483484Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                 id  \\\n0  718d41cd997b2b44b0685ac54aa55bd8   \n1  c926e7b0717202618a10dd907d4b4c39   \n2  d38ef5bf1fb82b410026ed82c8a44cae   \n3  b6b5589a98fdccd208dc752bac853993   \n4  82396a18fa9812bfec4d3ecb7ae60905   \n\n                                             context                title  \\\n0  Thủ tướng Trung Quốc là nhân vật lãnh đạo chín...           Trung Quốc   \n1  có 23 quốc gia không có lực lượng quân đội, ba...                        \n2  Raymondienne (hay Raymonde Dien) sinh ngày 13 ...         Raymondienne   \n3  Cúp cờ vua thế giới là tên gọi một số giải đấu...  Cúp cờ vua thế giới   \n4  Đỉnh núi nằm ở phần trung tâm của dãy núi Đại ...              Shkhara   \n\n                                            question            category  \\\n0                         Thủ tướng Trung Quốc là gì  PARTIAL_ANNOTATION   \n1                     Đất nước nào không có quân đội     FULL_ANNOTATION   \n2  Pháp tấn công xâm lược Việt Nam vào ngày tháng...   FALSE_LONG_ANSWER   \n3                     Cờ vua còn có tên gọi nào khác   FALSE_LONG_ANSWER   \n4                           Núi nào cao nhất châu âu     FULL_ANNOTATION   \n\n   answer  short_candidate_start  \\\n0       1                    NaN   \n1       1                   53.0   \n2       0                    NaN   \n3       0                    NaN   \n4       1                   73.0   \n\n                                     short_candidate  \n0                                               None  \n1  Costa Rica, Iceland, Panama, Micronesia, Quần ...  \n2                                               None  \n3                                               None  \n4                                         núi Elbrus  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>context</th>\n      <th>title</th>\n      <th>question</th>\n      <th>category</th>\n      <th>answer</th>\n      <th>short_candidate_start</th>\n      <th>short_candidate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>718d41cd997b2b44b0685ac54aa55bd8</td>\n      <td>Thủ tướng Trung Quốc là nhân vật lãnh đạo chín...</td>\n      <td>Trung Quốc</td>\n      <td>Thủ tướng Trung Quốc là gì</td>\n      <td>PARTIAL_ANNOTATION</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c926e7b0717202618a10dd907d4b4c39</td>\n      <td>có 23 quốc gia không có lực lượng quân đội, ba...</td>\n      <td></td>\n      <td>Đất nước nào không có quân đội</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>53.0</td>\n      <td>Costa Rica, Iceland, Panama, Micronesia, Quần ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d38ef5bf1fb82b410026ed82c8a44cae</td>\n      <td>Raymondienne (hay Raymonde Dien) sinh ngày 13 ...</td>\n      <td>Raymondienne</td>\n      <td>Pháp tấn công xâm lược Việt Nam vào ngày tháng...</td>\n      <td>FALSE_LONG_ANSWER</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6b5589a98fdccd208dc752bac853993</td>\n      <td>Cúp cờ vua thế giới là tên gọi một số giải đấu...</td>\n      <td>Cúp cờ vua thế giới</td>\n      <td>Cờ vua còn có tên gọi nào khác</td>\n      <td>FALSE_LONG_ANSWER</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>82396a18fa9812bfec4d3ecb7ae60905</td>\n      <td>Đỉnh núi nằm ở phần trung tâm của dãy núi Đại ...</td>\n      <td>Shkhara</td>\n      <td>Núi nào cao nhất châu âu</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>73.0</td>\n      <td>núi Elbrus</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"retrieval_model = SentenceTransformer('keepitreal/vietnamese-sbert')","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:41.485453Z","iopub.execute_input":"2024-01-27T06:38:41.485780Z","iopub.status.idle":"2024-01-27T06:38:49.235343Z","shell.execute_reply.started":"2024-01-27T06:38:41.485748Z","shell.execute_reply":"2024-01-27T06:38:49.234346Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading .gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f48259d455f4b46afbc8f3db0a38718"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9377c150e04c48c2a5e4690ff2282220"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading README.md:   0%|          | 0.00/3.90k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90997568dc7e487f906a7c8f2c6bf853"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading added_tokens.json:   0%|          | 0.00/17.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"063d62691e3b4c5483cb7ab32299a953"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d683764aa9ae426eb4e0565610a02268"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/752 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13c7c4c81d6d444d900b5d964061164c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae9ddf61c77e4e92926738fbb62311c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)aluation_results.csv:   0%|          | 0.00/767 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"048a60f677d747d8a71d7d10c35757ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0dbdd66cf59414f8dfc16511630f75b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05e3c25584744c3c8fbc5cb3fbe98f95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a7fe417e6ec45808ae27fe3162ab09b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2b9a95246e943f18923cb8e9e0b1998"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21fedafa1a114b91afda1d6ff0d0a2e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f949ae2dec7b435c8786510d61b1d90d"}},"metadata":{}}]},{"cell_type":"code","source":"train_data = df","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:49.236484Z","iopub.execute_input":"2024-01-27T06:38:49.236766Z","iopub.status.idle":"2024-01-27T06:38:49.246665Z","shell.execute_reply.started":"2024-01-27T06:38:49.236740Z","shell.execute_reply":"2024-01-27T06:38:49.245841Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_examples = []\nfor i in range(len(train_data)):\n    train_examples.append(InputExample(texts=[train_data.iloc[i]['question'], train_data.iloc[i]['context']], label=float(train_data.iloc[i]['answer'])))","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:49.247964Z","iopub.execute_input":"2024-01-27T06:38:49.248271Z","iopub.status.idle":"2024-01-27T06:38:53.521583Z","shell.execute_reply.started":"2024-01-27T06:38:49.248246Z","shell.execute_reply":"2024-01-27T06:38:53.520608Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_examples[0].texts","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:53.522790Z","iopub.execute_input":"2024-01-27T06:38:53.523087Z","iopub.status.idle":"2024-01-27T06:38:53.529474Z","shell.execute_reply.started":"2024-01-27T06:38:53.523061Z","shell.execute_reply":"2024-01-27T06:38:53.528591Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['Thủ tướng Trung Quốc là gì',\n 'Thủ tướng Trung Quốc là nhân vật lãnh đạo chính phủ , chủ trì Quốc vụ viện gồm bốn phó thủ tướng cùng người đứng đầu các bộ và uỷ ban cấp bộ . Chủ tịch nước đương nhiệm là Tập Cận Bình , ông cũng là Tổng Bí thư của Đảng Cộng sản Trung Quốc và Chủ tịch Quân uỷ Trung Quốc , do vậy ông là lãnh đạo tối cao của Trung Quốc .']"},"metadata":{}}]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\ntrain_loss = losses.CosineSimilarityLoss(retrieval_model)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:53.535732Z","iopub.execute_input":"2024-01-27T06:38:53.536046Z","iopub.status.idle":"2024-01-27T06:38:53.542122Z","shell.execute_reply.started":"2024-01-27T06:38:53.536014Z","shell.execute_reply":"2024-01-27T06:38:53.541240Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"retrieval_model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:53.543140Z","iopub.execute_input":"2024-01-27T06:38:53.543404Z","iopub.status.idle":"2024-01-27T06:46:00.278948Z","shell.execute_reply.started":"2024-01-27T06:38:53.543378Z","shell.execute_reply":"2024-01-27T06:46:00.277837Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e2d48542664d8180f3ceb17789b5e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1304 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e009b70dbea44eec960966b7b9cc6a0a"}},"metadata":{}}]},{"cell_type":"code","source":"your_model_path = 'Retrievel_model'\nretrieval_model.save(your_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:46:00.280611Z","iopub.execute_input":"2024-01-27T06:46:00.280916Z","iopub.status.idle":"2024-01-27T06:46:01.044054Z","shell.execute_reply.started":"2024-01-27T06:46:00.280889Z","shell.execute_reply":"2024-01-27T06:46:01.043209Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"passages = []\nfor i in range(len(df)):\n    passages.append(df.iloc[i]['context'])\ncorpus_embeddings = retrieval_model.encode(passages, convert_to_tensor=True, show_progress_bar=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:46:01.045134Z","iopub.execute_input":"2024-01-27T06:46:01.045420Z","iopub.status.idle":"2024-01-27T06:46:57.501980Z","shell.execute_reply.started":"2024-01-27T06:46:01.045395Z","shell.execute_reply":"2024-01-27T06:46:57.500959Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/652 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2da4e6922a241cfbc2cc94ac264976d"}},"metadata":{}}]},{"cell_type":"code","source":"def search(query):\n    print(\"Input question:\", query)\n\n    question_embedding = retrieval_model.encode(query, convert_to_tensor=True)\n    question_embedding = question_embedding.cuda()\n    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k =10)\n    hits = hits[0]\n\n    print(\"---------------------------\")\n    print(\"Top-10 retrieval hits\")\n    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n    for hit in hits[0:10]:\n        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:46:57.503403Z","iopub.execute_input":"2024-01-27T06:46:57.503702Z","iopub.status.idle":"2024-01-27T06:46:57.510333Z","shell.execute_reply.started":"2024-01-27T06:46:57.503676Z","shell.execute_reply":"2024-01-27T06:46:57.509416Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"question = train_data.iloc[100].question","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:46:57.511470Z","iopub.execute_input":"2024-01-27T06:46:57.511729Z","iopub.status.idle":"2024-01-27T06:46:57.523401Z","shell.execute_reply.started":"2024-01-27T06:46:57.511706Z","shell.execute_reply":"2024-01-27T06:46:57.522597Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"search(question)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:46:57.524529Z","iopub.execute_input":"2024-01-27T06:46:57.525128Z","iopub.status.idle":"2024-01-27T06:46:57.605381Z","shell.execute_reply.started":"2024-01-27T06:46:57.525102Z","shell.execute_reply":"2024-01-27T06:46:57.604397Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Input question: nhạc sĩ nào là tác giả của bài hát nổi tiếng dáng đứng bến tre\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"764af83bef5a45aca1d90774b5337cf9"}},"metadata":{}},{"name":"stdout","text":"---------------------------\nTop-10 retrieval hits\n\t0.738\tHoàn cảnh ra đời . Nhạc sĩ Nguyễn Văn Tý nổi tiếng với nhiều ca khúc như : \" Dư âm \" ( 1949 ) , \" Một khúc tâm tình của người Hà Tĩnh \" ( 1974 ) , \" Mẹ yêu con \" , \" Người đi xây hồ Kẻ Gỗ \" ( 1976 ) , \" Tấm áo chiến sĩ mẹ vá năm xưa \" ( 1973 ) , \" Dáng đứng Bến Tre \" ( 1980 ) , \" Cô đi nuôi dạy trẻ \" ( 1980 ) .\n\t0.727\tTừ hơn 500 năm về trước trong bài thơ \" Lộ nhập Vân Đồn \" , Nguyễn Trãi đã lần đầu tiên ca ngợi vịnh Hạ Long là \" kỳ quan đá dựng giữa trời cao \" .\n\t0.718\tHoa Lư là quê hương của nghệ thuật sân khấu chèo mà người sáng lập là bà Phạm Thị Trân , một vũ ca tài ba trong hoàng cung nhà Đinh . Đây là loại hình sân khấu được hình thành sớm nhất và tiêu biểu nhất của Việt Nam . Qua truyền thuyết về pháp sư Văn Du Tường thời Đinh dùng mưu diệt quỷ Xương Cuồng ở Bạch Hạc cho thấy nghệ thuật xiếc , tạp kỹ như đi trên dây , đánh đu , trồng cây chuối đã xuất hiện .\n\t0.709\tTóm tắt tác phẩm . Hồi thứ nhất . Vũ Như Tô , một kiến trúc sư thiên tài , bị hôn quân Lê Tương Dực bắt xây dựng Cửu Trùng đài để làm nơi hưởng lạc , vui chơi với các cung nữ .\n\t0.703\tLịch sử . Trong chiến tranh Tống–Việt lần thứ hai . Đang đêm , Lý Thường Kiệt cho người vào đền thờ Trương Hống , Trương Hát ở phía nam bờ sông Như Nguyệt , giả làm thần đọc vang bài thơ trên .\n\t0.701\tNhận định . Và Trần Lựu được thờ làm thành hoàng ở đình Thanh Hà , phố Ngõ Gạch ( Hà Nội ) là một nhân vật có thật - một vị tướng tài ba có đóng góp quan trọng vào thắng lợi của cuộc kháng chiến chống quân Minh xâm lược đầu thế kỷ XV .\n\t0.696\tVăn hoá . Xã Mai Phụ là quê hương của Mai Thúc Loan , vị Hoàng đế có công đánh đuổi quân xâm lược Nhà Đường , Tiến sĩ Nguyễn Chí Dũng - Uỷ viên BCH Trung ương Đảng , Nguyên Bí thư Tỉnh uỷ Ninh Thuận , Bộ trưởng Bộ Kế hoạch - đầu tư , ĐBQH khoá XII , XIII Làng Thu Hoạch , xã Thạch Châu là quê hương của nhà sử học Phan Phu Tiên nhà nghiên cứu văn học và nhà giáo nổi tiếng đầu đời Lê .\n\t0.696\tVăn hoá . Xã Mai Phụ là quê hương của Mai Thúc Loan , vị Hoàng đế có công đánh đuổi quân xâm lược Nhà Đường , Tiến sĩ Nguyễn Chí Dũng - Uỷ viên BCH Trung ương Đảng , Nguyên Bí thư Tỉnh uỷ Ninh Thuận , Bộ trưởng Bộ Kế hoạch - đầu tư , ĐBQH khoá XII , XIII Làng Thu Hoạch , xã Thạch Châu là quê hương của nhà sử học Phan Phu Tiên nhà nghiên cứu văn học và nhà giáo nổi tiếng đầu đời Lê .\n\t0.688\tTừ hơn 500 năm về trước trong bài thơ \" Lộ nhập Vân Đồn \" , Nguyễn Trãi đã lần đầu tiên ca ngợi vịnh Hạ Long là \" kỳ quan đá dựng giữa trời cao \" . Năm 1962 Bộ Văn hoá - Thông tin ( Việt Nam ) đã xếp hạng vịnh Hạ Long là di tích danh thắng cấp quốc gia đồng thời quy hoạch vùng bảo vệ .\n\t0.686\tSự nghiệp . Năm 2003 , ông cho làm tượng đài cá ba sa ngay ngã ba sông Châu Đốc , nơi phát tích của nghề nuôi cá ba sa trong lồng , bè và tượng đài bông lúa đặt trước trụ sở Uỷ ban nhân dân tỉnh An Giang .\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Rerank model","metadata":{}},{"cell_type":"code","source":"cross_encoder = CrossEncoder('keepitreal/vietnamese-sbert', num_labels=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:46:57.606679Z","iopub.execute_input":"2024-01-27T06:46:57.607418Z","iopub.status.idle":"2024-01-27T06:47:03.257280Z","shell.execute_reply.started":"2024-01-27T06:46:57.607382Z","shell.execute_reply":"2024-01-27T06:47:03.256236Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/752 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b77ce6ff06742acade181614cc2916d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f85a91315c014085827741e1c445abcd"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at keepitreal/vietnamese-sbert and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb9e66a588154ca5857539145c5486ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c19444fe60924cb6b209eae5d08e02af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"093f2962dd894498a3bfb4d915a9466d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading added_tokens.json:   0%|          | 0.00/17.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b96916f6ab148fea04d10b1e43874c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7576ddc90ef495aa4c504c9889094f4"}},"metadata":{}}]},{"cell_type":"code","source":"cross_encoder.fit(train_dataloader=train_dataloader, epochs=1, warmup_steps=100)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-01-27T06:47:03.258791Z","iopub.execute_input":"2024-01-27T06:47:03.259080Z","iopub.status.idle":"2024-01-27T06:53:40.942485Z","shell.execute_reply.started":"2024-01-27T06:47:03.259055Z","shell.execute_reply":"2024-01-27T06:53:40.941639Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9452e4712ab48dfb20fa1738c3ec380"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1304 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd8bed1e407343c191b9cc282df8f4f9"}},"metadata":{}},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"}]},{"cell_type":"code","source":"cross_encoder.save('Rerank_model')","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:40.944051Z","iopub.execute_input":"2024-01-27T06:53:40.944358Z","iopub.status.idle":"2024-01-27T06:53:41.699170Z","shell.execute_reply.started":"2024-01-27T06:53:40.944331Z","shell.execute_reply":"2024-01-27T06:53:41.698155Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Reading Comprehension","metadata":{}},{"cell_type":"code","source":"index = 13\ntext1 = df.iloc[index].context\ntext2 = df.iloc[index].question\nprint(text2)\nprint(text1)\nscore = cross_encoder.predict([text1, text2])\nprint(f\"Similar score: {score}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:41.700405Z","iopub.execute_input":"2024-01-27T06:53:41.700762Z","iopub.status.idle":"2024-01-27T06:53:41.741921Z","shell.execute_reply.started":"2024-01-27T06:53:41.700727Z","shell.execute_reply":"2024-01-27T06:53:41.741051Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Chiếc cầu rộng nhất thế giới\nCầu có chiều dài 483 m, rộng 7 m gồm 11 nhịp với kinh phí đầu tư 104 tỷ đồng. Công ty xây dựng công trình giao thông 61 tiếp tục trải bê-tông nhựa để kịp thông xe kỹ thuật vào ngày 26 tháng 4 năm ??. Cầu Bình Phước thuộc Dự án đường xuyên Á, thiết kế cho 4 làn xe ô tô và 2 làn xe thô sơ, tải trọng 30 tấn. Hiện nay đã có cầu Bình Phước 2 nằm song song với cầu Bình Phước cũ, tải trọng từ 25 - 30 tấn.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bca4d6ace79457081269f6eeda9d8b6"}},"metadata":{}},{"name":"stdout","text":"Similar score: 0.7752692103385925\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:41.743274Z","iopub.execute_input":"2024-01-27T06:53:41.743674Z","iopub.status.idle":"2024-01-27T06:53:41.758004Z","shell.execute_reply.started":"2024-01-27T06:53:41.743637Z","shell.execute_reply":"2024-01-27T06:53:41.757037Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                                 id  \\\n0  718d41cd997b2b44b0685ac54aa55bd8   \n1  c926e7b0717202618a10dd907d4b4c39   \n2  d38ef5bf1fb82b410026ed82c8a44cae   \n3  b6b5589a98fdccd208dc752bac853993   \n4  82396a18fa9812bfec4d3ecb7ae60905   \n\n                                             context                title  \\\n0  Thủ tướng Trung Quốc là nhân vật lãnh đạo chín...           Trung Quốc   \n1  có 23 quốc gia không có lực lượng quân đội, ba...                        \n2  Raymondienne (hay Raymonde Dien) sinh ngày 13 ...         Raymondienne   \n3  Cúp cờ vua thế giới là tên gọi một số giải đấu...  Cúp cờ vua thế giới   \n4  Đỉnh núi nằm ở phần trung tâm của dãy núi Đại ...              Shkhara   \n\n                                            question            category  \\\n0                         Thủ tướng Trung Quốc là gì  PARTIAL_ANNOTATION   \n1                     Đất nước nào không có quân đội     FULL_ANNOTATION   \n2  Pháp tấn công xâm lược Việt Nam vào ngày tháng...   FALSE_LONG_ANSWER   \n3                     Cờ vua còn có tên gọi nào khác   FALSE_LONG_ANSWER   \n4                           Núi nào cao nhất châu âu     FULL_ANNOTATION   \n\n   answer  short_candidate_start  \\\n0       1                    NaN   \n1       1                   53.0   \n2       0                    NaN   \n3       0                    NaN   \n4       1                   73.0   \n\n                                     short_candidate  \n0                                               None  \n1  Costa Rica, Iceland, Panama, Micronesia, Quần ...  \n2                                               None  \n3                                               None  \n4                                         núi Elbrus  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>context</th>\n      <th>title</th>\n      <th>question</th>\n      <th>category</th>\n      <th>answer</th>\n      <th>short_candidate_start</th>\n      <th>short_candidate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>718d41cd997b2b44b0685ac54aa55bd8</td>\n      <td>Thủ tướng Trung Quốc là nhân vật lãnh đạo chín...</td>\n      <td>Trung Quốc</td>\n      <td>Thủ tướng Trung Quốc là gì</td>\n      <td>PARTIAL_ANNOTATION</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c926e7b0717202618a10dd907d4b4c39</td>\n      <td>có 23 quốc gia không có lực lượng quân đội, ba...</td>\n      <td></td>\n      <td>Đất nước nào không có quân đội</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>53.0</td>\n      <td>Costa Rica, Iceland, Panama, Micronesia, Quần ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d38ef5bf1fb82b410026ed82c8a44cae</td>\n      <td>Raymondienne (hay Raymonde Dien) sinh ngày 13 ...</td>\n      <td>Raymondienne</td>\n      <td>Pháp tấn công xâm lược Việt Nam vào ngày tháng...</td>\n      <td>FALSE_LONG_ANSWER</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6b5589a98fdccd208dc752bac853993</td>\n      <td>Cúp cờ vua thế giới là tên gọi một số giải đấu...</td>\n      <td>Cúp cờ vua thế giới</td>\n      <td>Cờ vua còn có tên gọi nào khác</td>\n      <td>FALSE_LONG_ANSWER</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>82396a18fa9812bfec4d3ecb7ae60905</td>\n      <td>Đỉnh núi nằm ở phần trung tâm của dãy núi Đại ...</td>\n      <td>Shkhara</td>\n      <td>Núi nào cao nhất châu âu</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>73.0</td>\n      <td>núi Elbrus</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_qa = df[df['category'] == 'FULL_ANNOTATION']","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:41.759164Z","iopub.execute_input":"2024-01-27T06:53:41.759507Z","iopub.status.idle":"2024-01-27T06:53:41.776132Z","shell.execute_reply.started":"2024-01-27T06:53:41.759473Z","shell.execute_reply":"2024-01-27T06:53:41.775417Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df_qa","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:41.777321Z","iopub.execute_input":"2024-01-27T06:53:41.778227Z","iopub.status.idle":"2024-01-27T06:53:41.799254Z","shell.execute_reply.started":"2024-01-27T06:53:41.778194Z","shell.execute_reply":"2024-01-27T06:53:41.798264Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                                     id  \\\n1      c926e7b0717202618a10dd907d4b4c39   \n4      82396a18fa9812bfec4d3ecb7ae60905   \n9      ee8a272bcefac7b96037a1571984b1f8   \n16     c51604b25a641b2665c4ce1784b8cc6a   \n23     04bf774a424133d27c34895960937b87   \n...                                 ...   \n20826  be7f71e3927632f3f3f2afa68ff1b91b   \n20836  5769cee0487a9674306d271854244b39   \n20840  e47529f7ade3caaaa60096137798c6ff   \n20842  935572f75515f8d8ab831648589899c6   \n20852  508022f540c39fe31511f594748759bc   \n\n                                                 context  \\\n1      có 23 quốc gia không có lực lượng quân đội, ba...   \n4      Đỉnh núi nằm ở phần trung tâm của dãy núi Đại ...   \n9      Lịch sử . Ai Cập bị La Mã chiếm năm 30 TCN , v...   \n16     Các nghiên cứu lịch sử cho thấy Cổ Am rất có t...   \n23     Phục Hưng (tiếng Pháp: \"Renaissance\", , , từ \"...   \n...                                                  ...   \n20826  Tiền giấy xuất hiện . Năm 1396 thời Trần Thuận...   \n20836  Máy quay phim: Phát minh kỳ diệu của anh em Lu...   \n20840  Ngũ Hành Sơn hay núi Non Nước là một danh thắn...   \n20842  Trận Như Nguyệt là một trận đánh lớn diễn ra ở...   \n20852  Trong thần thoại Hy Lạp , \" Eros \" là vị thần ...   \n\n                            title  \\\n1                                   \n4                         Shkhara   \n9                      Alexandria   \n16                          Cổ Am   \n23                      Phục Hưng   \n...                           ...   \n20826  Tiền tệ Đại Việt thời Trần   \n20836                               \n20840                Ngũ Hành Sơn   \n20842             Trận Như Nguyệt   \n20852                        Eros   \n\n                                                question         category  \\\n1                         Đất nước nào không có quân đội  FULL_ANNOTATION   \n4                               Núi nào cao nhất châu âu  FULL_ANNOTATION   \n9      Thành phố nào là thủ phủ của Ai Cập trong đế q...  FULL_ANNOTATION   \n16     Ai là người đứng đầu trong cuộc chống lại chín...  FULL_ANNOTATION   \n23           Nền văn hoá Phục Hưng bắt nguồn từ nước nào  FULL_ANNOTATION   \n...                                                  ...              ...   \n20826  vua nào cho phát hành tiền giấy đầu tiên ở việ...  FULL_ANNOTATION   \n20836             Ai là người phát minh ra máy quay phim  FULL_ANNOTATION   \n20840  tên quận nào ở đà nẵng cũng là tên một danh th...  FULL_ANNOTATION   \n20842  trận đánh nào có ý nghĩa quyết định trong khán...  FULL_ANNOTATION   \n20852  trong thần thoại hy lạp vị thần tình yêu có tê...  FULL_ANNOTATION   \n\n       answer  short_candidate_start  \\\n1           1                   53.0   \n4           1                   73.0   \n9           1                   48.0   \n16          1                   65.0   \n23          1                  205.0   \n...       ...                    ...   \n20826       1                   54.0   \n20836       1                   37.0   \n20840       1                  285.0   \n20842       1                    0.0   \n20852       1                   28.0   \n\n                                         short_candidate  \n1      Costa Rica, Iceland, Panama, Micronesia, Quần ...  \n4                                             núi Elbrus  \n9                                             Alexandria  \n16                                               Lê Chân  \n23                                                     Ý  \n...                                                  ...  \n20826                                          Hồ Quý Ly  \n20836                         anh em Lumière Phương Hiền  \n20840                                  quận Ngũ Hành Sơn  \n20842                                    Trận Như Nguyệt  \n20852                                               Eros  \n\n[4989 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>context</th>\n      <th>title</th>\n      <th>question</th>\n      <th>category</th>\n      <th>answer</th>\n      <th>short_candidate_start</th>\n      <th>short_candidate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>c926e7b0717202618a10dd907d4b4c39</td>\n      <td>có 23 quốc gia không có lực lượng quân đội, ba...</td>\n      <td></td>\n      <td>Đất nước nào không có quân đội</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>53.0</td>\n      <td>Costa Rica, Iceland, Panama, Micronesia, Quần ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>82396a18fa9812bfec4d3ecb7ae60905</td>\n      <td>Đỉnh núi nằm ở phần trung tâm của dãy núi Đại ...</td>\n      <td>Shkhara</td>\n      <td>Núi nào cao nhất châu âu</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>73.0</td>\n      <td>núi Elbrus</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ee8a272bcefac7b96037a1571984b1f8</td>\n      <td>Lịch sử . Ai Cập bị La Mã chiếm năm 30 TCN , v...</td>\n      <td>Alexandria</td>\n      <td>Thành phố nào là thủ phủ của Ai Cập trong đế q...</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>48.0</td>\n      <td>Alexandria</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>c51604b25a641b2665c4ce1784b8cc6a</td>\n      <td>Các nghiên cứu lịch sử cho thấy Cổ Am rất có t...</td>\n      <td>Cổ Am</td>\n      <td>Ai là người đứng đầu trong cuộc chống lại chín...</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>65.0</td>\n      <td>Lê Chân</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>04bf774a424133d27c34895960937b87</td>\n      <td>Phục Hưng (tiếng Pháp: \"Renaissance\", , , từ \"...</td>\n      <td>Phục Hưng</td>\n      <td>Nền văn hoá Phục Hưng bắt nguồn từ nước nào</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>205.0</td>\n      <td>Ý</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20826</th>\n      <td>be7f71e3927632f3f3f2afa68ff1b91b</td>\n      <td>Tiền giấy xuất hiện . Năm 1396 thời Trần Thuận...</td>\n      <td>Tiền tệ Đại Việt thời Trần</td>\n      <td>vua nào cho phát hành tiền giấy đầu tiên ở việ...</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>54.0</td>\n      <td>Hồ Quý Ly</td>\n    </tr>\n    <tr>\n      <th>20836</th>\n      <td>5769cee0487a9674306d271854244b39</td>\n      <td>Máy quay phim: Phát minh kỳ diệu của anh em Lu...</td>\n      <td></td>\n      <td>Ai là người phát minh ra máy quay phim</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>37.0</td>\n      <td>anh em Lumière Phương Hiền</td>\n    </tr>\n    <tr>\n      <th>20840</th>\n      <td>e47529f7ade3caaaa60096137798c6ff</td>\n      <td>Ngũ Hành Sơn hay núi Non Nước là một danh thắn...</td>\n      <td>Ngũ Hành Sơn</td>\n      <td>tên quận nào ở đà nẵng cũng là tên một danh th...</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>285.0</td>\n      <td>quận Ngũ Hành Sơn</td>\n    </tr>\n    <tr>\n      <th>20842</th>\n      <td>935572f75515f8d8ab831648589899c6</td>\n      <td>Trận Như Nguyệt là một trận đánh lớn diễn ra ở...</td>\n      <td>Trận Như Nguyệt</td>\n      <td>trận đánh nào có ý nghĩa quyết định trong khán...</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>Trận Như Nguyệt</td>\n    </tr>\n    <tr>\n      <th>20852</th>\n      <td>508022f540c39fe31511f594748759bc</td>\n      <td>Trong thần thoại Hy Lạp , \" Eros \" là vị thần ...</td>\n      <td>Eros</td>\n      <td>trong thần thoại hy lạp vị thần tình yêu có tê...</td>\n      <td>FULL_ANNOTATION</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>Eros</td>\n    </tr>\n  </tbody>\n</table>\n<p>4989 rows × 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"contexts = []\nquestions = []\nanswers = []\nfor i in range(df_qa.shape[0]):\n    line = df_qa.iloc[i]\n    answer = {}\n    contexts.append(line['context'])\n    questions.append(line['question'])\n    answer['answer_start'] = int(line['short_candidate_start'])\n    answer['text'] = line['short_candidate']\n    answers.append(answer)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:41.800584Z","iopub.execute_input":"2024-01-27T06:53:41.800873Z","iopub.status.idle":"2024-01-27T06:53:42.212232Z","shell.execute_reply.started":"2024-01-27T06:53:41.800849Z","shell.execute_reply":"2024-01-27T06:53:42.211274Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(len(contexts))\nprint(len(questions))\nprint(len(answers))","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:42.213484Z","iopub.execute_input":"2024-01-27T06:53:42.213828Z","iopub.status.idle":"2024-01-27T06:53:42.219757Z","shell.execute_reply.started":"2024-01-27T06:53:42.213795Z","shell.execute_reply":"2024-01-27T06:53:42.218832Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"4989\n4989\n4989\n","output_type":"stream"}]},{"cell_type":"code","source":"print(contexts[0], end='\\n\\n')\nprint(questions[0], end='\\n\\n')\nprint(answers[0], end='\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:42.220879Z","iopub.execute_input":"2024-01-27T06:53:42.221260Z","iopub.status.idle":"2024-01-27T06:53:42.231415Z","shell.execute_reply.started":"2024-01-27T06:53:42.221234Z","shell.execute_reply":"2024-01-27T06:53:42.230529Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"có 23 quốc gia không có lực lượng quân đội, bao gồm: Costa Rica, Iceland, Panama, Micronesia, Quần đảo Marshall, và Vatican...\n\nĐất nước nào không có quân đội\n\n{'answer_start': 53, 'text': 'Costa Rica, Iceland, Panama, Micronesia, Quần đảo Marshall, và Vatican...'}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def add_end_idx(answers, contexts):\n    # loop through each answer-context pair\n    for answer, context in zip(answers, contexts):\n        # gold_text refers to the answer we are expecting to find in context\n        gold_text = answer['text']\n        # we already know the start index\n        start_idx = answer['answer_start']\n        # and ideally this would be the end index...\n        end_idx = start_idx + len(gold_text)\n\n        # ...however, sometimes squad answers are off by a character or two\n        if context[start_idx:end_idx] == gold_text:\n            # if the answer is not off :)\n            answer['answer_end'] = end_idx\n        else:\n            for n in [1, 2]:\n                if context[start_idx-n:end_idx-n] == gold_text:\n                    # this means the answer is off by 'n' tokens\n                    answer['answer_start'] = start_idx - n\n                    answer['answer_end'] = end_idx - n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:42.232538Z","iopub.execute_input":"2024-01-27T06:53:42.232806Z","iopub.status.idle":"2024-01-27T06:53:42.242152Z","shell.execute_reply.started":"2024-01-27T06:53:42.232781Z","shell.execute_reply":"2024-01-27T06:53:42.241131Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"add_end_idx(answers, contexts)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:42.243350Z","iopub.execute_input":"2024-01-27T06:53:42.243605Z","iopub.status.idle":"2024-01-27T06:53:42.260823Z","shell.execute_reply.started":"2024-01-27T06:53:42.243582Z","shell.execute_reply":"2024-01-27T06:53:42.260042Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print(contexts[0], end='\\n\\n')\nprint(questions[0], end='\\n\\n')\nprint(answers[0], end='\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:42.262020Z","iopub.execute_input":"2024-01-27T06:53:42.262369Z","iopub.status.idle":"2024-01-27T06:53:42.271655Z","shell.execute_reply.started":"2024-01-27T06:53:42.262297Z","shell.execute_reply":"2024-01-27T06:53:42.270769Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"có 23 quốc gia không có lực lượng quân đội, bao gồm: Costa Rica, Iceland, Panama, Micronesia, Quần đảo Marshall, và Vatican...\n\nĐất nước nào không có quân đội\n\n{'answer_start': 53, 'text': 'Costa Rica, Iceland, Panama, Micronesia, Quần đảo Marshall, và Vatican...', 'answer_end': 126}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import XLMRobertaTokenizerFast, XLMRobertaForQuestionAnswering\n\ntokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\nmodel = XLMRobertaForQuestionAnswering.from_pretrained(\"xlm-roberta-base\")","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:42.272835Z","iopub.execute_input":"2024-01-27T06:53:42.273135Z","iopub.status.idle":"2024-01-27T06:53:52.981696Z","shell.execute_reply.started":"2024-01-27T06:53:42.273103Z","shell.execute_reply":"2024-01-27T06:53:52.980975Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecf62da7a3a949a3b050d2d0d6416359"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"836f6fb63672460e89162ca49e7470ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c34f76a32a54af0b40198f2bc024afb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"417a195b8a87488c807025f44b2a2e91"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_encodings = tokenizer(contexts, questions, truncation=True, padding=True, return_offsets_mapping=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:52.982828Z","iopub.execute_input":"2024-01-27T06:53:52.983093Z","iopub.status.idle":"2024-01-27T06:53:54.345223Z","shell.execute_reply.started":"2024-01-27T06:53:52.983070Z","shell.execute_reply":"2024-01-27T06:53:54.344320Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"print(\"Tokens:\", train_encodings[\"input_ids\"][0])\nprint(\"Attention Mask:\", train_encodings[\"attention_mask\"][0])","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:54.346286Z","iopub.execute_input":"2024-01-27T06:53:54.346587Z","iopub.status.idle":"2024-01-27T06:53:54.352396Z","shell.execute_reply.started":"2024-01-27T06:53:54.346560Z","shell.execute_reply":"2024-01-27T06:53:54.351476Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Tokens: [0, 524, 1105, 10895, 3529, 687, 524, 9611, 6372, 29225, 25738, 4, 8609, 33256, 12, 23047, 92051, 4, 114749, 4, 149674, 4, 37992, 86, 3478, 4, 10022, 249994, 19, 54520, 157530, 4, 544, 103302, 27, 2, 2, 137229, 3042, 3941, 687, 524, 29225, 25738, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nAttention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"# In ra chiều dài của các thành phần\nprint(\"Number of input_ids:\", len(train_encodings[\"input_ids\"]))\nprint(\"Number of attention_mask:\", len(train_encodings[\"attention_mask\"]))","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:54.353983Z","iopub.execute_input":"2024-01-27T06:53:54.354436Z","iopub.status.idle":"2024-01-27T06:53:54.364781Z","shell.execute_reply.started":"2024-01-27T06:53:54.354310Z","shell.execute_reply":"2024-01-27T06:53:54.363847Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Number of input_ids: 4989\nNumber of attention_mask: 4989\n","output_type":"stream"}]},{"cell_type":"code","source":"len(train_encodings[\"input_ids\"][0])","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:54.365889Z","iopub.execute_input":"2024-01-27T06:53:54.366255Z","iopub.status.idle":"2024-01-27T06:53:54.376966Z","shell.execute_reply.started":"2024-01-27T06:53:54.366221Z","shell.execute_reply":"2024-01-27T06:53:54.376142Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"512"},"metadata":{}}]},{"cell_type":"code","source":"def add_token_positions(encodings, answers):\n    # initialize lists to contain the token indices of answer start/end\n    start_positions = []\n    end_positions = []\n    for i in range(len(answers)):\n        # append start/end token position using char_to_token method\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n\n        # if start position is None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n        # end position cannot be found, char_to_token found space, so shift one token forward\n        go_back = 1\n        while end_positions[-1] is None:\n            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end']-go_back)\n            go_back +=1\n    # update our encodings object with the new token-based start/end positions\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\n# apply function to our data\nadd_token_positions(train_encodings, answers)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:54.378225Z","iopub.execute_input":"2024-01-27T06:53:54.378532Z","iopub.status.idle":"2024-01-27T06:53:54.407336Z","shell.execute_reply.started":"2024-01-27T06:53:54.378507Z","shell.execute_reply":"2024-01-27T06:53:54.406545Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"print(\"Tokens:\", train_encodings[\"input_ids\"][0])\nprint(\"Attention Mask:\", train_encodings[\"attention_mask\"][0])\nprint(\"Start Positions:\", train_encodings[\"start_positions\"][0])\nprint(\"End Positions:\", train_encodings[\"end_positions\"][0])","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:54.408302Z","iopub.execute_input":"2024-01-27T06:53:54.408589Z","iopub.status.idle":"2024-01-27T06:53:54.415736Z","shell.execute_reply.started":"2024-01-27T06:53:54.408565Z","shell.execute_reply":"2024-01-27T06:53:54.414913Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Tokens: [0, 524, 1105, 10895, 3529, 687, 524, 9611, 6372, 29225, 25738, 4, 8609, 33256, 12, 23047, 92051, 4, 114749, 4, 149674, 4, 37992, 86, 3478, 4, 10022, 249994, 19, 54520, 157530, 4, 544, 103302, 27, 2, 2, 137229, 3042, 3941, 687, 524, 29225, 25738, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nAttention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nStart Positions: 15\nEnd Positions: 34\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nclass SquadDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\ntrain_dataset = SquadDataset(train_encodings)\n# val_dataset = SquadDataset(val_encodings)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:54.421573Z","iopub.execute_input":"2024-01-27T06:53:54.421854Z","iopub.status.idle":"2024-01-27T06:53:54.428063Z","shell.execute_reply.started":"2024-01-27T06:53:54.421812Z","shell.execute_reply":"2024-01-27T06:53:54.427171Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import AdamW\nfrom tqdm import tqdm\n\n# setup GPU/CPU\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n# move model over to detected device\nmodel.to(device)\n# activate training mode of model\nmodel.train()\n# initialize adam optimizer with weight decay (reduces chance of overfitting)\noptim = AdamW(model.parameters(), lr=5e-5)\n\n# initialize data loader for training data\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\nfor epoch in range(3):\n    # set model to train mode\n    model.train()\n    # setup loop (we use tqdm for the progress bar)\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        # initialize calculated gradients (from prev step)\n        optim.zero_grad()\n        # pull all the tensor batches required for training\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        # train model on batch and return outputs (incl. loss)\n        outputs = model(input_ids, attention_mask=attention_mask,\n                        start_positions=start_positions,\n                        end_positions=end_positions)\n        # extract loss\n        loss = outputs[0]\n        # calculate loss for every parameter that needs grad update\n        loss.backward()\n        # update parameters\n        optim.step()\n        # print relevant info to progress bar\n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:53:54.428938Z","iopub.execute_input":"2024-01-27T06:53:54.429202Z","iopub.status.idle":"2024-01-27T07:07:35.326747Z","shell.execute_reply.started":"2024-01-27T06:53:54.429180Z","shell.execute_reply":"2024-01-27T07:07:35.325796Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 0: 100%|██████████| 312/312 [04:33<00:00,  1.14it/s, loss=1.44] \nEpoch 1: 100%|██████████| 312/312 [04:33<00:00,  1.14it/s, loss=0.257]\nEpoch 2: 100%|██████████| 312/312 [04:33<00:00,  1.14it/s, loss=0.17] \n","output_type":"stream"}]},{"cell_type":"code","source":"model_path = 'QA/models'\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:07:35.328032Z","iopub.execute_input":"2024-01-27T07:07:35.328399Z","iopub.status.idle":"2024-01-27T07:07:37.699858Z","shell.execute_reply.started":"2024-01-27T07:07:35.328367Z","shell.execute_reply":"2024-01-27T07:07:37.698950Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"('QA/models/tokenizer_config.json',\n 'QA/models/special_tokens_map.json',\n 'QA/models/sentencepiece.bpe.model',\n 'QA/models/added_tokens.json',\n 'QA/models/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# Chắc chắn rằng cả model và tensors đều ở trên cùng một thiết bị (GPU)\nimport torch.nn.functional as F\n\nmodel.to('cuda:0')\n\ni = 101\n\ncontext = contexts[i]\nquestion = questions[i]\ntrue_answers = answers[i]['text']\n\n# Mã hóa context và question\nencoding = tokenizer(context, question, return_tensors=\"pt\")\n\n# Di chuyển encoding lên GPU\nencoding = {key: value.to('cuda:0') for key, value in encoding.items()}\n\n# Dự đoán\nwith torch.no_grad():\n    output = model(**encoding)\n\n# Lấy vị trí có xác suất lớn nhất\nstart_probs = F.softmax(output['start_logits'], dim=1)\nend_probs = F.softmax(output['end_logits'], dim=1)\n\n# Lấy vị trí có xác suất lớn nhất\nstart_index = torch.argmax(start_probs)\nend_index = torch.argmax(end_probs) + 1\n\n# Lấy độ tin cậy\nconfidence = start_probs[0][start_index].item() * end_probs[0][end_index-1].item()\n\n# Lấy câu trả lời dự đoán\nanswer = tokenizer.decode(encoding[\"input_ids\"][0][start_index:end_index])\n\nprint(f\"Question: {question}\")\nprint(f\"Context: {context}\")\nprint(f\"True Answer: {true_answers}\")\nprint(\"Predicted Answer:\", answer)\nprint(f\"Confidence: {round(confidence*100, 2)}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:07:37.701296Z","iopub.execute_input":"2024-01-27T07:07:37.701676Z","iopub.status.idle":"2024-01-27T07:07:37.746106Z","shell.execute_reply.started":"2024-01-27T07:07:37.701641Z","shell.execute_reply":"2024-01-27T07:07:37.745230Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Question: nhất gia bán thiên hạ là lời khen tặng của vua tự đức dành cho người dân làng nào\nContext: Truyền thống khoa bảng . Làng được vua Tự Đức ban tặng lời vàng : \" \" Mộ Trạch nhất gia bán thiên hạ \" \" ( Mộ Trạch tài năng bằng nửa thiên hạ ) .\nTrue Answer: Mộ Trạch\nPredicted Answer: Mộ Trạch\nConfidence: 99.28%\n","output_type":"stream"}]},{"cell_type":"code","source":"def search_context(query):\n#     print(\"Input question:\", query)\n\n    question_embedding = retrieval_model.encode(query, convert_to_tensor=True)\n    question_embedding = question_embedding.cuda()\n    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k = 200)\n    hits = hits[0]\n        \n    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n    cross_scores = cross_encoder.predict(cross_inp)\n    for idx in range(len(cross_scores)):\n        hits[idx]['cross-score'] = cross_scores[idx]\n    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n#     print(hits)\n#     for i in range(10):\n#         print(f\"Score: {round(hits[i]['cross-score'], 2)}  - {passages[hits[i]['corpus_id']]}\") \n#         print()\n    return passages[hits[0]['corpus_id']]\n\ndef search_answer(question, context):\n    encoding = tokenizer(context, question, return_tensors=\"pt\")\n    encoding = {key: value.to('cuda:0') for key, value in encoding.items()}\n    with torch.no_grad():\n        output = model(**encoding)\n\n    # Lấy vị trí có xác suất lớn nhất\n    start_probs = F.softmax(output['start_logits'], dim=1)\n    end_probs = F.softmax(output['end_logits'], dim=1)\n\n    # Lấy vị trí có xác suất lớn nhất\n    start_index = torch.argmax(start_probs)\n    end_index = torch.argmax(end_probs) + 1\n    answer = tokenizer.decode(encoding[\"input_ids\"][0][start_index:end_index])\n    \n    print(f\"Question: {question}\")\n    print(f\"Context: {context}\")\n#     print(f\"True Answer: {true_answers}\")\n    print(\"Predicted Answer:\", answer)\n#     print(f\"Confidence: {round(confidence*100, 2)}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:25:00.449420Z","iopub.execute_input":"2024-01-27T07:25:00.450338Z","iopub.status.idle":"2024-01-27T07:25:00.460675Z","shell.execute_reply.started":"2024-01-27T07:25:00.450293Z","shell.execute_reply":"2024-01-27T07:25:00.459734Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"id=1\ncontext = search_context(test_data[id]['question'])\nsearch_answer(test_data[id]['question'], context)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T07:25:41.958803Z","iopub.execute_input":"2024-01-27T07:25:41.959731Z","iopub.status.idle":"2024-01-27T07:25:43.274562Z","shell.execute_reply.started":"2024-01-27T07:25:41.959693Z","shell.execute_reply":"2024-01-27T07:25:43.273526Z"},"trusted":true},"execution_count":76,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc5879c6755142e996ffcd2e5abb311c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"432fb6a644bc404eaaf77db3f323234c"}},"metadata":{}},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Question: Tổng thống Hoa Kỳ thứ 45 là ai\nContext: Barack Hussein Obama II ( IPA : ; sinh ngày 4 tháng 8 năm 1961 ) là tổng thống thứ 44 của Hoa Kỳ từ năm 2009 đến năm 2017 . Ông là người Mỹ gốc Phi đầu tiên được bầu vào chức vụ này . Lớn lên ở Honolulu , Hawaii , cá nhân ông thừa hưởng những nền văn hoá Phi-Âu-Á-Mỹ của thế giới từ thuở thiếu thời , Obama tốt nghiệp Viện Đại học Columbia và Trường Luật Viện Đại học Harvard , nơi ông từng là chủ tịch \" Harvard Law Review \" .\nPredicted Answer: Barack Hussein Obama II\n","output_type":"stream"}]}]}